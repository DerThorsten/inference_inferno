\documentclass{article}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{hyperref}
\setlength\parindent{0pt}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}



\section{Dataset}
A sample from the Drosophila medulla prepared using FIB-SEM imaging.
The dataset is from \url{https://github.com/janelia-flyem/neuroproof_examples}.
It contains precomputed super-voxels and dense ground truth.

For fast experimentation I extracted 2D slices from the 3D data.

\section{Boundary/Face Prediction}

Use a handful of filters results to accumulate
statistics for each face / edge.

The filters are :
\begin{itemize}
\item hessian of gaussian eigenvalues
\item structure tensor eigenvalues
\item$ \ldots$
\end{itemize}


With these features a unstructured random 
forest is trained to predict a ``merge/not-merge''
probability for each edge.



\section{Features for Structured Learning}
First step is a simple (quantile based)
normalization of the raw features 
which have been used for the unstructured learning.
The result should be features in -1,1 or 0,1.

The random forest probabilities should also be used
as a feature. No normalization is needed here.

To make the features more powerful, we try
to emulate the kernel trick by approximating an
rbf kernel or a polynomial kernel.
Currently I am using hand chosen products
between normalized features and random forest probabilities.
In the future I want to try the following:

\begin{itemize}
\item Nystroem Approximation of kernel
\end{itemize}


\section{Structured Learning}

Let $G$ be a graph $G = (V,E)$.
For each variable  we have a node label $y_i \in |V|$.
Therefore, each variable has as many labels as there are variables
such that we can encode any partitioning.
The multicut objective as a graph labeling problem is given by:
\begin{align*}
    y_{map} &=   
        \textbf{argmin}_{y} 
                \sum_{ uv \in E} 
                    \beta_{uv} \cdot \big[ y_u \neq y_v \big]
\end{align*}

A negative $\beta_{uv}$ is a prior/desire that node $u$ and $v$ should be
in different clusters, a positive $\beta_{uv}$ is a prior/desire to
put $u$ and $v$ in the same cluster.

To learn this objective we replace with $\beta_{uv}$
with $w^Tx_{uv}$ where $w$ is the structured learning weight 
vector, and $x_{uv}$ the feature vector for the edge $uv$.
With  $\beta_{uv} = w^Tx_{uv}$ we get:
\begin{align*}
    y_{map} &=   
        \textbf{argmin}_{y} 
                \sum_{ uv \in E} 
                    w^Tx_{uv} \cdot \big[ y_u \neq y_v \big] 
\end{align*}
Let us define $\textbf{Y}(w)$ as a function returning the MAP labeling for a given weight:
\begin{align*}
    \textbf{Y}(w) & =   
        \textbf{argmin}_{y} 
                \sum_{ uv \in E} 
                    w^Tx_{uv} \cdot \big[ y_u \neq y_v \big]
\end{align*}

It is easy to see that $\textbf{Y}(w) = \textbf{Y}( \alpha \cdot w) \quad\forall \alpha > 0$ . This means that
the multicut objective is invariant with respect to scaling of $w$.




Let $DS$ be the dataset.
We will use $i$ to index a certain training example.
\begin{align*}
    w_{opt} = 
        argmin_{w} 
        \hspace{0.2cm} \norm{w}^2 + 
        C \cdot 
        \sum_{i \in |DS|}  
        \Delta(y^i_{gt},\textbf{Y}^i(w))
\end{align*}

Weights with a large norm $\norm{w}^2$ are ``usually'' considered as potentially over-fitting.
A small positive $C$ has the effect that $\norm{w_{opt}}^2$ should be ``small''.
And  ``usually'' one associates small weights with a low risk of over-fitting.
But this is not true for the multicut objective.
Since $\textbf{Y}(w) = \textbf{Y}( \alpha \cdot w) \quad\forall \alpha > 0$ we
can just take potentially over-fitting weights with a large norm and scale them
with a small $\alpha$. 

In other words, $C$ does not put any meaningful restrictions on the weights.
But if we fix a single entry in $w$ to be $1$, let us say the weight which is associated with the ``constant-$1$-feature'',
we fix that problem.

This is the same as the following objective with \textbf{un-constraint} weights:

\begin{align*}
    \textbf{Y}(w) & =   
        \textbf{argmin}_{y} 
                \sum_{ uv \in E} 
                    ( w^Tx_{uv} + 1)\cdot \big[ y_u \neq y_v \big] 
\end{align*}



\subsection{Weight Initialization}

I use a sub-gradient ssvm with hamming loss on the edges to
initialize the weights.
Even without such an initialization I can minimize the structured loss
functions.

\subsection{Structured Loss Functions}
I did most experiments with 
variation of information (\url{https://en.wikipedia.org/wiki/Variation_of_information})
and rand index (\url{https://en.wikipedia.org/wiki/Rand_index}).
But I also tried f-score (\url{https://en.wikipedia.org/wiki/F1_score}).

\subsection{(stochastic) Gradient Decent Algorithm}

I am using the ``first'' algorithm again.
\begin{itemize}
\item iterate over dataset in random order
\item get perturbed weights and remember the noise added in each perturbation
\item compute loss for each perturbation
\item Compute weighted sum between added noise and loss
and use that as gradient
\end{itemize}

\subsection{Momentum}

Momentum means, in each gradient step we add a bit of the
old gradient to the current gradient (
\url{http://www.willamette.edu/~gorr/classes/cs449/momrate.html} ).
Using momentum this is not needed at all(!).

\end{document}
